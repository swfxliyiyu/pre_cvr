{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "            \n",
    "def property_count(data):\n",
    "    total_count = data.shape[0]\n",
    "    property_dict = defaultdict(int)\n",
    "    for item in data['item_property_list']:\n",
    "        for item2 in str(item).split(';'):\n",
    "            property_dict[item2] += 1\n",
    "    property_value = np.array(list(property_dict.values()))\n",
    "    property_keys = np.array(list(property_dict.keys()))\n",
    "    property_sort_idx = np.argsort(property_value)[::-1]\n",
    "    \n",
    "    m_sum = 0\n",
    "    XL_bin = [] # 75%\n",
    "    L_bin = [] # 75% - 50%\n",
    "    M_bin = [] # 50% - 25%\n",
    "    S_bin = [] # 25% - 0%\n",
    "    for idx in property_sort_idx:\n",
    "        m_sum += property_value[idx]\n",
    "        if m_sum / total_count < 0.25:\n",
    "            XL_bin.append(int(property_keys[idx]))\n",
    "        elif m_sum / total_count < 0.5:\n",
    "            L_bin.append(int(property_keys[idx]))\n",
    "        elif m_sum / total_count < 0.75:\n",
    "            M_bin.append(int(property_keys[idx]))\n",
    "        else:\n",
    "            S_bin.append(int(property_keys[idx]))\n",
    "    return XL_bin, L_bin, M_bin, S_bin\n",
    "      \n",
    "# data preprocess\n",
    "def datapreprocess(train_file_path, test_file_path):\n",
    "    data = pd.read_csv(train_file_path, sep=' ', header=0)\n",
    "    test_data = pd.read_csv(test_file_path, sep=' ', header=0)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print('befor preprocess')\n",
    "    print('train shape:', data.shape)\n",
    "    print('test shape:', test_data.shape)\n",
    "    \n",
    "    def gen_click_feature(data):\n",
    "        '''\n",
    "        util function\n",
    "        '''\n",
    "        def time2str(v):\n",
    "            return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(v))\n",
    "        def _gen(data, group_by_cols, new_name):\n",
    "            groupby = data.groupby(group_by_cols).size().reset_index().rename(columns={0: new_name})\n",
    "            data = pd.merge(data, groupby, 'left', on=group_by_cols)\n",
    "            return data\n",
    "            \n",
    "        '''\n",
    "        add ['time','day','hour']\n",
    "        '''\n",
    "        data['time'] = data['context_timestamp'].apply(time2str)\n",
    "        data['day'] = data['time'].apply(lambda x:int(x[8:10]))\n",
    "        data['hour'] = data['time'].apply(lambda x:int(x[11:13]))\n",
    "        \n",
    "        '''\n",
    "        add ['user_click_day','user_click_hour']\n",
    "        '''\n",
    "        data = _gen(data, ['user_id', 'day'], 'user_click_day')\n",
    "        data = _gen(data, ['user_id', 'day', 'hour'], 'user_click_hour')\n",
    "\n",
    "        '''\n",
    "        add ['shop_click_day','shop_click_hour']\n",
    "        '''\n",
    "        data = _gen(data, ['shop_id', 'day'], 'shop_click_day')\n",
    "        data = _gen(data, ['shop_id', 'day', 'hour'], 'shop_click_hour')\n",
    "        \n",
    "        '''\n",
    "        add ['item_click_day','item_click_hour']\n",
    "        '''\n",
    "        data = _gen(data, ['item_id', 'day'], 'item_click_day')\n",
    "        data = _gen(data, ['item_id', 'day', 'hour'], 'item_click_hour')\n",
    "        \n",
    "        del data['time']\n",
    "        return data\n",
    "    \n",
    "    def gen_category_feature(data, test_data):\n",
    "        # util func\n",
    "#         def category_count(data):\n",
    "#             category_dict = defaultdict(set)\n",
    "#             for item in data['item_category_list']:\n",
    "#                 for idx, item2 in enumerate(str(item).split(';')):\n",
    "#                     category_dict['cate_{}'.format(idx)].add(int(item2))\n",
    "#             for key in category_dict.keys():\n",
    "#                 category_dict[key].add(-1)\n",
    "#             return category_dict\n",
    "        \n",
    "        con_data = pd.concat([data.drop(columns=['is_trade']), test_data], ignore_index=True)\n",
    "        con_data['item_cate_1'] = con_data['item_category_list'].apply(\n",
    "            lambda x: int(str(x).split(';')[1]) if 1 < len(str(x).split(';')) else -1)\n",
    "        con_data['item_cate_2'] = con_data['item_category_list'].apply(\n",
    "            lambda x: int(str(x).split(';')[2]) if 2 < len(str(x).split(';')) else -1)\n",
    "        onehot_ed = OneHotEncoder()\n",
    "        label_ed = LabelEncoder()\n",
    "        \n",
    "        label_ed.fit(con_data['item_cate_1'])\n",
    "        onehot_ed.fit(label_ed.transform(con_data['item_cate_1']).reshape(-1, 1))\n",
    "        onehot_array = onehot_ed.transform(label_ed.transform(con_data['item_cate_1']).reshape(-1, 1)).toarray()\n",
    "        onehot_cate_1 = pd.DataFrame(onehot_array, columns=['cate1_' + str(x) for x in range(onehot_array.shape[1])])\n",
    "        del onehot_array\n",
    "        \n",
    "        label_ed.fit(con_data['item_cate_2'])\n",
    "        onehot_ed.fit(label_ed.transform(con_data['item_cate_2']).reshape(-1, 1)) \n",
    "        onehot_array = onehot_ed.transform(label_ed.transform(con_data['item_cate_2']).reshape(-1, 1)).toarray()\n",
    "        onehot_cate_2 = pd.DataFrame(onehot_array, columns=['cate2_' + str(x) for x in range(onehot_array.shape[1])])\n",
    "        del onehot_array\n",
    "\n",
    "        con_data = pd.concat([con_data, onehot_cate_1, onehot_cate_2], axis=1)\n",
    "        del con_data['item_cate_1']\n",
    "        del con_data['item_cate_2']\n",
    "        data = pd.concat([con_data.iloc[0:data.shape[0]], data['is_trade']], axis=1)\n",
    "        test_data = con_data.iloc[data.shape[0]:]\n",
    "        \n",
    "        return data, test_data\n",
    "    \n",
    "    #data = gen_click_feature(data)\n",
    "    #test_data = gen_click_feature(test_data)\n",
    "    #data, test_data = gen_category_feature(data, test_data)\n",
    "    \n",
    "    print('after preprocess')\n",
    "    print('train shape:', data.shape)\n",
    "    print('test shape:', test_data.shape)\n",
    "    drop_columns = ['instance_id','item_category_list','item_property_list','predict_category_property']\n",
    "    return data.drop(columns = drop_columns), test_data['instance_id'], test_data.drop(columns = drop_columns)\n",
    "\n",
    "\n",
    "def trainAsubmission(file_path, test_path, write_path, model):\n",
    "    data, instance_id, test_data = datapreprocess(file_path, test_path)\n",
    "    x, y = data.values[:,:-1], data.values[:,-1]\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    # train loss\n",
    "    y_pred = model.predict_proba(x)\n",
    "    print('train loss ',log_loss(y, y_pred))\n",
    "    \n",
    "    #make_submission\n",
    "    result = pd.DataFrame()\n",
    "    result['instance_id'] = instance_id\n",
    "    result['predicted_score'] = model.predict_proba(test_data.values)[:,1]\n",
    "    result.to_csv(write_path, sep=' ', index=False)\n",
    "    \n",
    "\n",
    "# def make_submission(file_path, test_path, write_path, trained_model):\n",
    "#     data, instance_id, test_data = datapreprocess(file_path, test_path)\n",
    "    \n",
    "\n",
    "# data,instance_id,test_data = datapreprocess('train.txt', 'test.txt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befor preprocess\n",
      "train shape: (478111, 27)\n",
      "test shape: (18371, 26)\n",
      "after preprocess\n",
      "train shape: (478111, 27)\n",
      "test shape: (18371, 26)\n",
      "train loss  0.0891840973511807\n"
     ]
    }
   ],
   "source": [
    "trainAsubmission('train.txt', 'test.txt', 'res_xgboost_1.txt', XGBClassifier(silent=1,objective='binary:logistic',seed=1203))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        1\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "         ..\n",
       "478081    0\n",
       "478082    0\n",
       "478083    0\n",
       "478084    0\n",
       "478085    0\n",
       "478086    0\n",
       "478087    0\n",
       "478088    0\n",
       "478089    0\n",
       "478090    0\n",
       "478091    0\n",
       "478092    0\n",
       "478093    0\n",
       "478094    0\n",
       "478095    0\n",
       "478096    0\n",
       "478097    0\n",
       "478098    0\n",
       "478099    0\n",
       "478100    0\n",
       "478101    0\n",
       "478102    0\n",
       "478103    0\n",
       "478104    0\n",
       "478105    1\n",
       "478106    0\n",
       "478107    0\n",
       "478108    0\n",
       "478109    0\n",
       "478110    0\n",
       "Name: is_trade, Length: 478111, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_trade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.txt', sep=' ', header=0)\n",
    "test_data = pd.read_csv('test.txt', sep=' ', header=0)\n",
    "con_data = pd.concat([data.drop(columns=['is_trade']),test_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "def category_count(data):\n",
    "    category_dict = defaultdict(set)\n",
    "    for item in data['item_category_list']:\n",
    "        for idx, item2 in enumerate(str(item).split(';')):\n",
    "            category_dict['cate_{}'.format(idx)].add(item2)\n",
    "    return category_dict\n",
    "            \n",
    "def property_count(data):\n",
    "    total_count = data.shape[0]\n",
    "    property_dict = defaultdict(int)\n",
    "    for item in data['item_property_list']:\n",
    "        for item2 in str(item).split(';'):\n",
    "            property_dict[item2] += 1\n",
    "    property_value = np.array(list(property_dict.values()))\n",
    "    property_keys = np.array(list(property_dict.keys()))\n",
    "    property_sort_idx = np.argsort(property_value)[::-1]\n",
    "    \n",
    "    m_sum = 0\n",
    "    XL_bin = [] # 75%\n",
    "    L_bin = [] # 75% - 50%\n",
    "    M_bin = [] # 50% - 25%\n",
    "    S_bin = [] # 25% - 0%\n",
    "    for idx in property_sort_idx:\n",
    "        m_sum += property_value[idx]\n",
    "        if m_sum / total_count < 0.25:\n",
    "            XL_bin.append(int(property_keys[idx]))\n",
    "        else if m_sum / total_count < 0.5:\n",
    "            L_bin.append(int(property_keys[idx]))\n",
    "        else if m_sum / total_count < 0.75:\n",
    "            M_bin.append(int(property_keys[idx]))\n",
    "        else:\n",
    "            S_bin.append(int(property_keys[idx]))\n",
    "    return XL_bin, L_bin, M_bin, S_bin\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
